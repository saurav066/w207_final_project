{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-06T15:54:38.501178Z",
     "iopub.status.busy": "2021-11-06T15:54:38.500286Z",
     "iopub.status.idle": "2021-11-06T15:54:38.519451Z",
     "shell.execute_reply": "2021-11-06T15:54:38.518668Z",
     "shell.execute_reply.started": "2021-11-06T15:54:38.501122Z"
    }
   },
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# General libraries.\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "import sklearn\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report,f1_score\n",
    "\n",
    "# SK-learn library for importing the newsgroup data.\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "#nltk.download('wordnet')\n",
    "#conda install -c conda-forge wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "## Data Collected from https://www.yelp.com/dataset/download\n",
    "## File size for Yelp Review - 6.94 GB\n",
    "## 1 Million records were extracted from the JSON and converted to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-06T15:54:39.75973Z",
     "iopub.status.busy": "2021-11-06T15:54:39.759272Z",
     "iopub.status.idle": "2021-11-06T15:54:39.771005Z",
     "shell.execute_reply": "2021-11-06T15:54:39.76994Z",
     "shell.execute_reply.started": "2021-11-06T15:54:39.759695Z"
    }
   },
   "outputs": [],
   "source": [
    "row_size = 1000000\n",
    "json_path_4_review = './yelp_academic_dataset_review.json'\n",
    "df = pd.read_json(json_path_4_review, lines=True,\n",
    "                 dtype={'review_id':str,'name':str,\n",
    "                        'business_id':str,'stars':int,\n",
    "                        'date':str,'text':str,'useful':int,\n",
    "                        'funny':int,'cool':int},\n",
    "                  chunksize=row_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-06T15:56:34.555142Z",
     "iopub.status.busy": "2021-11-06T15:56:34.554831Z",
     "iopub.status.idle": "2021-11-06T15:56:46.017912Z",
     "shell.execute_reply": "2021-11-06T15:56:46.016904Z",
     "shell.execute_reply.started": "2021-11-06T15:56:34.555112Z"
    }
   },
   "outputs": [],
   "source": [
    "train_datasets = []\n",
    "dev_datasets = []\n",
    "test_datasets = []\n",
    "i=0\n",
    "for chunk in df:\n",
    "    if i==0:\n",
    "        train_datasets.append(chunk)\n",
    "    else:\n",
    "        break;\n",
    "    i=i+1\n",
    "\n",
    "train_dataset = pd.concat(train_datasets, ignore_index=True, join='outer', axis=0)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trimmed dataset with 1 million records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-06T15:56:46.020417Z",
     "iopub.status.busy": "2021-11-06T15:56:46.020071Z",
     "iopub.status.idle": "2021-11-06T15:56:46.035822Z",
     "shell.execute_reply": "2021-11-06T15:56:46.034668Z",
     "shell.execute_reply.started": "2021-11-06T15:56:46.020372Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_dataset.to_csv('yelp_academic_dataset_review.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading the dataset with 1 million records and splitting them in train and test set\n",
    "## Added this file creation since we were toggling between Local jupyter notebook, Kaggle and Colab. \n",
    "## So created the trimmed file created for portability and for reduced resource utilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=pd.read_csv('./yelp_academic_dataset_review.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-06T15:56:46.037783Z",
     "iopub.status.busy": "2021-11-06T15:56:46.037426Z",
     "iopub.status.idle": "2021-11-06T15:56:50.464599Z",
     "shell.execute_reply": "2021-11-06T15:56:50.463488Z",
     "shell.execute_reply.started": "2021-11-06T15:56:46.037735Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restaurant Ratings [4 5 2 1 3]\n",
      "No. of Reviews for each rating:\n",
      " 5    438341\n",
      "4    225903\n",
      "1    144169\n",
      "3    109154\n",
      "2     82433\n",
      "Name: stars, dtype: int64\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'shuffle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-e6fa68926908>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Sentiments\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'shuffle' is not defined"
     ]
    }
   ],
   "source": [
    "def sentimentGrouping(s):\n",
    "    if (s >= 4):\n",
    "        rating=1\n",
    "    elif (s == 3):\n",
    "        rating=0\n",
    "    else:\n",
    "        rating=-1\n",
    "    \n",
    "    return rating\n",
    "\n",
    "#Restaurant Ratings \n",
    "print(\"Restaurant Ratings\",pd.unique(train_dataset['stars']))\n",
    "print(\"No. of Reviews for each rating:\\n\",pd.value_counts(train_dataset['stars']))\n",
    "\n",
    "train_dataset['Sentiments'] = train_dataset.apply(lambda x: sentimentGrouping(x['stars']), axis=1)\n",
    "\n",
    "X = train_dataset[\"text\"]\n",
    "y = train_dataset[\"Sentiments\"]\n",
    "\n",
    "X,y = shuffle(X,y)\n",
    "\n",
    "train_data, dev_data, train_labels, dev_labels = train_test_split(X, y, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Let's Consider Rating 4 & 5 - Positive\n",
    "##Let's Consider Rating 3 - Neutral\n",
    "##Let's Consider Rating 1 & 2 - Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-06T15:56:50.475862Z",
     "iopub.status.busy": "2021-11-06T15:56:50.475415Z",
     "iopub.status.idle": "2021-11-06T15:56:50.726085Z",
     "shell.execute_reply": "2021-11-06T15:56:50.72522Z",
     "shell.execute_reply.started": "2021-11-06T15:56:50.475802Z"
    }
   },
   "outputs": [],
   "source": [
    "#Negative Sentiments\n",
    "neg_phrases = train_dataset[train_dataset.Sentiments == -1]\n",
    "neg_words = []\n",
    "for t in neg_phrases.text:\n",
    "    neg_words.append(t)\n",
    "negative_words = pd.Series(neg_words).str.cat(sep=' ')\n",
    "\n",
    "#Positive Sentiments\n",
    "positive_phrases = train_dataset[train_dataset.Sentiments == 1]\n",
    "positive_words = []\n",
    "for t in positive_phrases.text:\n",
    "    positive_words.append(t)\n",
    "pos_words = pd.Series(positive_words).str.cat(sep=' ')\n",
    "\n",
    "#Neutral Sentiments\n",
    "neutral_phrases = train_dataset[train_dataset.Sentiments == 0]\n",
    "neutral_words = []\n",
    "for t in neutral_phrases.text:\n",
    "    neutral_words.append(t)\n",
    "neu_words = pd.Series(neutral_words).str.cat(sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-06T15:56:50.728004Z",
     "iopub.status.busy": "2021-11-06T15:56:50.727732Z",
     "iopub.status.idle": "2021-11-06T15:57:03.481406Z",
     "shell.execute_reply": "2021-11-06T15:57:03.480707Z",
     "shell.execute_reply.started": "2021-11-06T15:56:50.727972Z"
    }
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "wordcloud = WordCloud(width=1600, height=800, max_font_size=200).generate(negative_words)\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-06T15:57:03.483209Z",
     "iopub.status.busy": "2021-11-06T15:57:03.482464Z",
     "iopub.status.idle": "2021-11-06T15:57:30.056618Z",
     "shell.execute_reply": "2021-11-06T15:57:30.055632Z",
     "shell.execute_reply.started": "2021-11-06T15:57:03.483169Z"
    }
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "wordcloud = WordCloud(width=1600, height=800, max_font_size=200).generate(pos_words)\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-06T15:57:30.058198Z",
     "iopub.status.busy": "2021-11-06T15:57:30.057933Z",
     "iopub.status.idle": "2021-11-06T15:57:38.475118Z",
     "shell.execute_reply": "2021-11-06T15:57:38.474239Z",
     "shell.execute_reply.started": "2021-11-06T15:57:30.058166Z"
    }
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "wordcloud = WordCloud(width=1600, height=800, max_font_size=200).generate(neu_words)\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-06T15:57:38.476561Z",
     "iopub.status.busy": "2021-11-06T15:57:38.47631Z",
     "iopub.status.idle": "2021-11-06T15:58:07.741272Z",
     "shell.execute_reply": "2021-11-06T15:58:07.740295Z",
     "shell.execute_reply.started": "2021-11-06T15:57:38.47653Z"
    }
   },
   "outputs": [],
   "source": [
    "vector = CountVectorizer(analyzer='word', stop_words='english')\n",
    "\n",
    "X_train = vector.fit_transform(train_data)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "train_tfidf = tfidf_transformer.fit_transform(X_train)\n",
    "\n",
    "X_dev = vector.transform(dev_data)\n",
    "dev_tfidf = tfidf_transformer.transform(X_dev)\n",
    "\n",
    "lr_clf = LogisticRegression(C=0.5,solver=\"liblinear\", multi_class=\"auto\",penalty=\"l2\",tol=0.015)\n",
    "lr_clf.fit(train_tfidf, train_labels)\n",
    "\n",
    "dev_labels_pred = lr_clf.predict(X_dev)\n",
    "    \n",
    "#mse = np.sqrt(sum((dev_labels - dev_labels_pred) ** 2))\n",
    "#print(\"Accuracy for Model Logistic Regression is %f\" % ( accuracy))\n",
    "print(\"F1 score is %f\" % (f1_score(dev_labels, dev_labels_pred, average='weighted')))\n",
    "#print(\"Number of vocabulary: \",len(vector_reduced.vocabulary_))\n",
    "\n",
    "print(classification_report(dev_labels,dev_labels_pred))\n",
    "print(\"Accuracy of Logistic Regression without Hyperparameter Tuning: %f\" % (accuracy_score(dev_labels,dev_labels_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-06T15:58:07.745759Z",
     "iopub.status.busy": "2021-11-06T15:58:07.745457Z",
     "iopub.status.idle": "2021-11-06T15:58:07.752122Z",
     "shell.execute_reply": "2021-11-06T15:58:07.750971Z",
     "shell.execute_reply.started": "2021-11-06T15:58:07.745726Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Number of vocabulary: \",len(vector.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-06T15:58:07.754826Z",
     "iopub.status.busy": "2021-11-06T15:58:07.754134Z",
     "iopub.status.idle": "2021-11-06T16:05:18.284934Z",
     "shell.execute_reply": "2021-11-06T16:05:18.283875Z",
     "shell.execute_reply.started": "2021-11-06T15:58:07.754775Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Model Parameter tuning\n",
    "#Trying different C value for both L1 and L2 simultaneously\n",
    "#Using L1 regression to reduce the vocabulary\n",
    "#Applying L2 regression on the reduced Vocabulary\n",
    "\n",
    "c=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\n",
    "\n",
    "\n",
    "f1_score_lst = []\n",
    "vocabularies = []\n",
    "\n",
    "for val in c:\n",
    "        \n",
    "    vector = CountVectorizer(analyzer='word', stop_words='english')\n",
    "    X_train = vector.fit_transform(train_data)\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    train_tfidf = tfidf_transformer.fit_transform(X_train)\n",
    "    \n",
    "    lr_clf = LogisticRegression(C=val,solver=\"liblinear\", multi_class=\"auto\", penalty=\"l1\",tol=.015)\n",
    "    lr_clf.fit(train_tfidf, train_labels)\n",
    "\n",
    "    features = vector.get_feature_names()\n",
    "\n",
    "    feature_dict = {}\n",
    "    feature_zero = []\n",
    "    vocabulary_exclude=[]\n",
    "\n",
    "    #Get the feature and increment them for each topic if they're zero\n",
    "    for topic in range(lr_clf.coef_.shape[0]):\n",
    "      for feature in range(lr_clf.coef_.shape[1]): \n",
    "        if (lr_clf.coef_[topic][feature] == 0):\n",
    "            feature_dict[feature] = feature_dict.get(feature,0) + 1\n",
    "\n",
    "    #Identify the vocabs for the features which are 0 coef for all stars\n",
    "    #and exclude them\n",
    "    for feature in feature_dict:\n",
    "      if feature_dict[feature] == lr_clf.coef_.shape[0]:\n",
    "        feature_zero.append(feature)\n",
    "        vocabulary_exclude.append(features[feature])\n",
    "\n",
    "    vector_reduced = CountVectorizer(analyzer='word',stop_words=vocabulary_exclude)\n",
    "    X_train = vector_reduced.fit_transform(train_data)\n",
    "    X_dev = vector_reduced.transform(dev_data)\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    train_tfidf = tfidf_transformer.fit_transform(X_train)\n",
    "    dev_tfidf = tfidf_transformer.fit_transform(X_dev)\n",
    "\n",
    "    lr_clf_2 = LogisticRegression(C=val,solver=\"liblinear\", multi_class=\"auto\", penalty=\"l2\",tol=.015)\n",
    "    lr_clf_2.fit(train_tfidf, train_labels)\n",
    "\n",
    "    dev_labels_pred = lr_clf_2.predict(dev_tfidf)\n",
    "\n",
    "    print(\"Regularization strength for L1 and L2 : \",val)\n",
    "    print(\"F1 score is %f\" % (f1_score(dev_labels, dev_labels_pred, average='weighted')))\n",
    "    len_vocab = len(vector_reduced.vocabulary_)\n",
    "    log_vocab = np.log(len_vocab)\n",
    "    print(\"Number of vocabulary: \",len(vector_reduced.vocabulary_))\n",
    "    print(\"Accuracy %f\" % accuracy_score(dev_labels,dev_labels_pred))\n",
    "    \n",
    "    \n",
    "    f1_score_lst.append(f1_score(dev_labels, dev_labels_pred, average='weighted'))\n",
    "    vocabularies.append(log_vocab)\n",
    "        \n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.plot(f1_score_lst, vocabularies)\n",
    "plt.title(\"F1 Score vs Log of vocabulary for L1 regression with reduced Vocabulary\")\n",
    "plt.xlabel('F1 Score')\n",
    "plt.ylabel('Log of vocabulary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-06T16:05:18.28733Z",
     "iopub.status.busy": "2021-11-06T16:05:18.286801Z",
     "iopub.status.idle": "2021-11-06T16:23:27.99725Z",
     "shell.execute_reply": "2021-11-06T16:23:27.996056Z",
     "shell.execute_reply.started": "2021-11-06T16:05:18.287281Z"
    }
   },
   "outputs": [],
   "source": [
    "#perform lemmatization\n",
    "\n",
    "def preprocessor_custom(doc):\n",
    "    \n",
    "    new_doc = str(doc)\n",
    "    \n",
    "    new_doc = re.sub(r'[^A-Za-z\\s]',r' ',new_doc)\n",
    "\n",
    "    new_doc = \" \".join([w.lower() for w in new_doc.split()])\n",
    "    \n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    new_doc = \" \".join([wordnet_lemmatizer.lemmatize(w, pos='v') for w in new_doc.split()])\n",
    "    \n",
    "    return new_doc\n",
    "\n",
    "c=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\n",
    "\n",
    "\n",
    "f1_score_lst = []\n",
    "vocabularies = []\n",
    "\n",
    "for val in c:\n",
    "\n",
    "    vector = CountVectorizer(analyzer='word', preprocessor=preprocessor_custom)\n",
    "    X_train = vector.fit_transform(train_data)\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    train_tfidf = tfidf_transformer.fit_transform(X_train)\n",
    "\n",
    "    lr_clf = LogisticRegression(C=val,solver=\"liblinear\", multi_class=\"auto\", penalty=\"l1\",tol=.015)\n",
    "    lr_clf.fit(train_tfidf, train_labels)\n",
    "\n",
    "    features = vector.get_feature_names()\n",
    "\n",
    "    feature_dict = {}\n",
    "    feature_zero = []\n",
    "    vocabulary_exclude=[]\n",
    "\n",
    "    #Get the feature and increment them for each topic if they're zero\n",
    "    for topic in range(lr_clf.coef_.shape[0]):\n",
    "      for feature in range(lr_clf.coef_.shape[1]): \n",
    "        if (lr_clf.coef_[topic][feature] == 0):\n",
    "            feature_dict[feature] = feature_dict.get(feature,0) + 1\n",
    "\n",
    "    #Identify the vocabs for the features which are 0 coef for all stars\n",
    "    #and exclude them\n",
    "    for feature in feature_dict:\n",
    "      if feature_dict[feature] == lr_clf.coef_.shape[0]:\n",
    "        feature_zero.append(feature)\n",
    "        vocabulary_exclude.append(features[feature])\n",
    "\n",
    "    vector_reduced = CountVectorizer(analyzer='word', stop_words=vocabulary_exclude)\n",
    "    X_train = vector_reduced.fit_transform(train_data)\n",
    "    X_dev = vector_reduced.transform(dev_data)\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    train_tfidf = tfidf_transformer.fit_transform(X_train)\n",
    "    dev_tfidf = tfidf_transformer.fit_transform(X_dev)\n",
    "\n",
    "    lr_clf_2 = LogisticRegression(C=val,solver=\"liblinear\", multi_class=\"auto\", penalty=\"l1\",tol=.015)\n",
    "    lr_clf_2.fit(train_tfidf, train_labels)\n",
    "\n",
    "    dev_labels_pred = lr_clf_2.predict(dev_tfidf)\n",
    "\n",
    "    print(\"F1 score is %f\" % (f1_score(dev_labels, dev_labels_pred, average='weighted')))\n",
    "    len_vocab = len(vector_reduced.vocabulary_)\n",
    "    log_vocab = np.log(len_vocab)\n",
    "    print(\"Number of vocabulary: \",len(vector_reduced.vocabulary_))\n",
    "    print(\"C Value for L1 and L2: \",val)\n",
    "    print(\"Accuracy %f\" % accuracy_score(dev_labels,dev_labels_pred))\n",
    "    f1_score_lst.append(f1_score(dev_labels, dev_labels_pred, average='weighted'))\n",
    "    vocabularies.append(log_vocab)\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.plot(f1_score_lst, vocabularies)\n",
    "plt.title(\"F1 Score vs Log of vocabulary for L1 and L2 regression with reduced Vocabulary\")\n",
    "plt.xlabel('F1 Score')\n",
    "plt.ylabel('Log of vocabulary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-06T16:23:27.999312Z",
     "iopub.status.busy": "2021-11-06T16:23:27.998991Z",
     "iopub.status.idle": "2021-11-06T16:23:28.008268Z",
     "shell.execute_reply": "2021-11-06T16:23:28.007456Z",
     "shell.execute_reply.started": "2021-11-06T16:23:27.999266Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-06T16:23:28.010144Z",
     "iopub.status.busy": "2021-11-06T16:23:28.009577Z",
     "iopub.status.idle": "2021-11-06T17:27:50.652121Z",
     "shell.execute_reply": "2021-11-06T17:27:50.650966Z",
     "shell.execute_reply.started": "2021-11-06T16:23:28.010102Z"
    }
   },
   "outputs": [],
   "source": [
    "xc=XGBClassifier(n_estimators=1000)\n",
    "xc.fit(train_tfidf, train_labels)\n",
    "\n",
    "xc_labels_pred = xc.predict(dev_tfidf)\n",
    "\n",
    "print(\"F1 score is %f\" % (f1_score(dev_labels, xc_labels_pred, average='weighted')))\n",
    "len_vocab = len(vector_reduced.vocabulary_)\n",
    "log_vocab = np.log(len_vocab)\n",
    "print(\"Number of vocabulary: \",len(vector_reduced.vocabulary_))\n",
    "print(\"C Value for L1 and L2: \",val)\n",
    "f1_score_lst.append(f1_score(dev_labels, xc_labels_pred, average='weighted'))\n",
    "print(\"Accuracy %f\" % accuracy_score(dev_labels,xc_labels_pred))\n",
    "vocabularies.append(log_vocab)\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.plot(f1_score_lst, vocabularies)\n",
    "plt.title(\"F1 Score vs Log of vocabulary for L1 and L2 regression with reduced Vocabulary\")\n",
    "plt.xlabel('F1 Score')\n",
    "plt.ylabel('Log of vocabulary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-06T17:27:50.654442Z",
     "iopub.status.busy": "2021-11-06T17:27:50.654097Z",
     "iopub.status.idle": "2021-11-06T17:27:50.659728Z",
     "shell.execute_reply": "2021-11-06T17:27:50.658837Z",
     "shell.execute_reply.started": "2021-11-06T17:27:50.654395Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-06T17:27:50.661689Z",
     "iopub.status.busy": "2021-11-06T17:27:50.661394Z",
     "iopub.status.idle": "2021-11-06T17:27:50.675443Z",
     "shell.execute_reply": "2021-11-06T17:27:50.674715Z",
     "shell.execute_reply.started": "2021-11-06T17:27:50.661649Z"
    }
   },
   "outputs": [],
   "source": [
    "rc=RandomForestClassifier(n_estimators=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-06T17:27:50.677466Z",
     "iopub.status.busy": "2021-11-06T17:27:50.676832Z",
     "iopub.status.idle": "2021-11-06T18:43:41.668125Z",
     "shell.execute_reply": "2021-11-06T18:43:41.666064Z",
     "shell.execute_reply.started": "2021-11-06T17:27:50.677419Z"
    }
   },
   "outputs": [],
   "source": [
    "rc.fit(train_tfidf, train_labels)\n",
    "\n",
    "rc_labels_pred = rc.predict(dev_tfidf)\n",
    "\n",
    "print(\"F1 score is %f\" % (f1_score(dev_labels, rc_labels_pred, average='weighted')))\n",
    "len_vocab = len(vector_reduced.vocabulary_)\n",
    "log_vocab = np.log(len_vocab)\n",
    "print(\"Number of vocabulary: \",len(vector_reduced.vocabulary_))\n",
    "print(\"C Value for L1 and L2: \",val)\n",
    "f1_score_lst.append(f1_score(dev_labels, rc_labels_pred, average='weighted'))\n",
    "print(\"Accuracy of Random Forest Classifier Model %f\" % accuracy_score(dev_labels,rc_labels_pred))\n",
    "\n",
    "vocabularies.append(log_vocab)\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.plot(f1_score_lst, vocabularies)\n",
    "plt.title(\"F1 Score vs Log of vocabulary for L1 and L2 regression with reduced Vocabulary\")\n",
    "plt.xlabel('F1 Score')\n",
    "plt.ylabel('Log of vocabulary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-06T18:43:41.669755Z",
     "iopub.status.busy": "2021-11-06T18:43:41.669374Z",
     "iopub.status.idle": "2021-11-06T18:43:41.674706Z",
     "shell.execute_reply": "2021-11-06T18:43:41.673636Z",
     "shell.execute_reply.started": "2021-11-06T18:43:41.669721Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-06T18:43:41.677145Z",
     "iopub.status.busy": "2021-11-06T18:43:41.676766Z",
     "iopub.status.idle": "2021-11-06T18:43:41.693019Z",
     "shell.execute_reply": "2021-11-06T18:43:41.692176Z",
     "shell.execute_reply.started": "2021-11-06T18:43:41.677108Z"
    }
   },
   "outputs": [],
   "source": [
    "svc=svm.SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-06T18:43:41.694816Z",
     "iopub.status.busy": "2021-11-06T18:43:41.694437Z",
     "iopub.status.idle": "2021-11-06T18:43:41.706628Z",
     "shell.execute_reply": "2021-11-06T18:43:41.705954Z",
     "shell.execute_reply.started": "2021-11-06T18:43:41.694785Z"
    }
   },
   "outputs": [],
   "source": [
    "param={'kernel':('linear','rbf'),'C':[1,10]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-06T18:43:41.710463Z",
     "iopub.status.busy": "2021-11-06T18:43:41.710042Z",
     "iopub.status.idle": "2021-11-06T18:43:41.720771Z",
     "shell.execute_reply": "2021-11-06T18:43:41.719845Z",
     "shell.execute_reply.started": "2021-11-06T18:43:41.710428Z"
    }
   },
   "outputs": [],
   "source": [
    "sv=GridSearchCV(svc,param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-06T18:43:41.722281Z",
     "iopub.status.busy": "2021-11-06T18:43:41.722047Z"
    }
   },
   "outputs": [],
   "source": [
    "sv.fit(train_tfidf, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(sv.cv_results_.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv_labels_pred=sv.predict(dev_tfidf)\n",
    "print(\"F1 score is %f\" % (f1_score(dev_labels, sv_labels_pred, average='weighted')))\n",
    "len_vocab = len(vector_reduced.vocabulary_)\n",
    "log_vocab = np.log(len_vocab)\n",
    "print(\"Number of vocabulary: \",len(vector_reduced.vocabulary_))\n",
    "print(\"C Value for L1 and L2: \",val)\n",
    "f1_score_lst.append(f1_score(dev_labels, sv_labels_pred, average='weighted'))\n",
    "print(\"Accuracy of Support Vector Machine Model %f\" % accuracy_score(dev_labels,sv_labels_pred))\n",
    "vocabularies.append(log_vocab)\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.plot(f1_score_lst, vocabularies)\n",
    "plt.title(\"F1 Score vs Log of vocabulary for L1 and L2 regression with reduced Vocabulary\")\n",
    "plt.xlabel('F1 Score')\n",
    "plt.ylabel('Log of vocabulary')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    \n",
    "    dtrain = xgb.DMatrix(train_tfidf, label=train_labels)\n",
    "    dtest = xgb.DMatrix(dev_tfidf, label=dev_labels)\n",
    "\n",
    "    param = {\n",
    "        \n",
    "        \"objective\": \"multi:softmax\",\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\"]),\n",
    "        \"num_class\":3,\n",
    "        }\n",
    "\n",
    "    if param[\"booster\"] == \"gbtree\":\n",
    "        param[\"max_depth\"] = trial.suggest_int(\"max_depth\", 1, 9)\n",
    "        param[\"eta\"] = trial.suggest_loguniform(\"eta\", 1e-8, 1.0)\n",
    "        param[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
    "  \n",
    "\n",
    "    # Add a callback for pruning.\n",
    "    pruning_callback = optuna.integration.XGBoostPruningCallback(trial,\"validation-auc\")\n",
    "    bst = xgb.train(param, dtrain, evals=[(dtest, \"validation\")], callbacks=[pruning_callback])\n",
    "    preds = bst.predict(dtest)\n",
    "   \n",
    "    pred_labels = np.rint(preds)\n",
    "    accuracy = sklearn.metrics.accuracy_score(dev_labels, pred_labels)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_NB = MultinomialNB()\n",
    "model_NB.fit(train_tfidf, train_labels) \n",
    "\n",
    "dev_labels_pred = model_NB.predict(dev_tfidf)\n",
    "print(\"Accuracy for Model MultinomialNB, is %f\" % (accuracy_score(dev_labels, dev_labels_pred)))\n",
    "print(classification_report(dev_labels,dev_labels_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc=LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels=lc.fit_transform(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_labels=lc.transform(dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.optimize(objective, n_trials=5)\n",
    "print(study.best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolution Neural Network for Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence, text\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding, LSTM\n",
    "from keras.layers import Conv1D, Flatten, MaxPooling1D\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "# Initialize wandb with your project name\n",
    "run = wandb.init(project='sentiment-analysis',\n",
    "                 config={  # and include hyperparameters and metadata\n",
    "                     \"learning_rate\": 0.005,\n",
    "                     \"epochs\": 5,\n",
    "                     \"batch_size\": 1024,\n",
    "                     \"loss_function\": \"sparse_categorical_crossentropy\",\n",
    "                     \"architecture\": \"CNN\",\n",
    "                     \"dataset\": \"YELP Reviews\"\n",
    "                 })\n",
    "\n",
    "config = wandb.config\n",
    "config.vocab_size = 1000\n",
    "config.maxlen = 1000\n",
    "config.batch_size=32\n",
    "config.embedding_dims = 10\n",
    "config.filters = 16\n",
    "config.kernel_size = 3\n",
    "config.hidden_dims = 250\n",
    "config.epochs = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence, text\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding, LSTM\n",
    "from keras.layers import Conv1D, Flatten, MaxPooling1D\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "# Initialize wandb with your project name\n",
    "run = wandb.init(project='sentiment-analysis',\n",
    "                 config={  # and include hyperparameters and metadata\n",
    "                     \"learning_rate\": 0.005,\n",
    "                     \"epochs\": 5,\n",
    "                     \"batch_size\": 1024,\n",
    "                     \"loss_function\": \"sparse_categorical_crossentropy\",\n",
    "                     \"architecture\": \"CNN\",\n",
    "                     \"dataset\": \"CIFAR-10\"\n",
    "                 })\n",
    "\n",
    "config = wandb.config\n",
    "config.vocab_size = 1000\n",
    "config.maxlen = 1000\n",
    "config.batch_size=32\n",
    "config.embedding_dims = 10\n",
    "config.filters = 16\n",
    "config.kernel_size = 3\n",
    "config.hidden_dims = 250\n",
    "config.epochs = 10\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=config.vocab_size)\n",
    "tokenizer.fit_on_texts(train_data)\n",
    "\n",
    "X_train = tokenizer.texts_to_matrix(train_data)\n",
    "X_test = tokenizer.texts_to_matrix(dev_data)\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=config.maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=config.maxlen)\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "Lr=LabelEncoder()\n",
    "Lr.fit(train_labels)\n",
    "train_l=Lr.transform(train_labels)\n",
    "test_l=Lr.transform(dev_labels)\n",
    "\n",
    "train_l=np_utils.to_categorical(train_l)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(config.vocab_size, config.embedding_dims, input_length=config.maxlen))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv1D(config.filters,config.kernel_size,padding='valid',activation='relu'))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Conv1D(config.filters,config.kernel_size,padding='valid',activation='relu'))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(config.hidden_dims,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3,activation='softmax'))\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=[['acc','auc']])\n",
    "history = model.fit(X_train, train_l, batch_size=config.batch_size, epochs=config.epochs)\n",
    "\n",
    "\n",
    "preds=model.predict(X_test)\n",
    "preds=np.argmax(preds,1)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy of CNN %f : \" % (accuracy_score(test_l, preds)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
